{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaseemAlkurdi97/Hardware-hacking-Roadmap/blob/main/Untitled7_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTFo6VawR2S_"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d \"rajharinir/lang8 \" --force\n",
        "! unzip \"/content/lang8 .zip\""
      ],
      "metadata": {
        "id": "PfPhs_FlSFUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm import tqdm \n",
        "import tensorflow as tf\n",
        "from  tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from  sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "OI0ehwngSdoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = open(\"/content/entries.train\")\n",
        "lines1 = f1.readlines()\n",
        "inp = [] \n",
        "tgt = []\n",
        "for i in lines1:\n",
        "    lst = i.split(\"\\t\")\n",
        "    if len(lst)>5  :     \n",
        "        inp.append(lst[-2]) \n",
        "        tgt.append(lst[-1]) \n",
        "\n",
        "df = pd.DataFrame() \n",
        "df[\"input\"] = inp \n",
        "df[\"output\"] =  tgt\n",
        "df[\"y\"] = list(\"1\"*len(inp))"
      ],
      "metadata": {
        "id": "O80y756lSiRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_spaces(text):\n",
        "    text = re.sub(r\" '(\\w)\",r\"'\\1\",text)\n",
        "    text = re.sub(r\" \\,\",\",\",text)\n",
        "    text = re.sub(r\" \\.+\",\".\",text)\n",
        "    text = re.sub(r\" \\!+\",\"!\",text)\n",
        "    text = re.sub(r\" \\?+\",\"?\",text)\n",
        "    text = re.sub(\" n't\",\"n't\",text)\n",
        "    text = re.sub(\"[\\(\\)\\;\\_\\^\\`\\/]\",\"\",text)\n",
        "    return text\n",
        "\n",
        "def decontract(text):\n",
        "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    return text\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(\"\\n\",\"\",text)\n",
        "    text = remove_spaces(text)   # REMOVING UNWANTED SPACES\n",
        "    text = re.sub(r\"\\.+\",\".\",text)\n",
        "    text = re.sub(r\"\\!+\",\"!\",text)\n",
        "    text = decontract(text)    # DECONTRACTION\n",
        "    text = re.sub(\"[^A-Za-z0-9 ]+\",\"\",text)\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "RH6PIVdRTlag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"processed_input\"] = df.input.apply(preprocess) \n",
        "df[\"processed_output\"] = df.output.apply(preprocess)\n",
        "df =df.drop([\"input\",\"output\"],axis=1)\n",
        "df = df[df.processed_input.notnull()]\n",
        "df = df[df.processed_output.notnull()]\n",
        "df = df.drop_duplicates()\n",
        "df"
      ],
      "metadata": {
        "id": "ibTWFBt-UQCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"processed_input\",\"processed_output\",\"y\"]].to_csv(\"./processed_data.csv\",index=False)"
      ],
      "metadata": {
        "id": "icx4RISKUkB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"./processed_data.csv\")\n",
        "df.columns = [\"enc_input\",\"dec_input\",\"y\"] \n",
        "df[\"dec_output\"] = df.dec_input\n",
        "df"
      ],
      "metadata": {
        "id": "mImRFnBNUlzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"dec_input\"]= \"<start> \" + df[\"dec_input\"]\n",
        "df[\"dec_output\"] =  df[\"dec_output\"] + \" <end>\" \n",
        "df"
      ],
      "metadata": {
        "id": "XAlxSOs6UwzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sampled = pd.concat((df[df.y==1].sample(frac= 0.2,random_state=1),df[df.y==2]))"
      ],
      "metadata": {
        "id": "kXsHmc_zUy_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ONCE THE DATA IS SAMPLED WE ARE SPLITTIND THE DATA IN TO TRAIN AND TEST\n",
        "df_train ,df_val = train_test_split(df_sampled,test_size=0.1,random_state = 3, stratify = df_sampled.y )"
      ],
      "metadata": {
        "id": "_N41UrzEVDN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## IN THE COLUMN WHICH HAS DECODER INPUTS ADDING \"<end>\" TOKEN TO BE LEARNED BY THE TOKENIZER\n",
        "df_train[\"dec_input\"].iloc[0]  = df_train.iloc[0][\"dec_input\"] + \" <end>\"\n",
        "df_train"
      ],
      "metadata": {
        "id": "_bX-NHf3VFhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## VALIDATION DATA\n",
        "df_val"
      ],
      "metadata": {
        "id": "gMunjjskVQXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n",
        "np.random.seed(5) \n",
        "df_test = df.loc[np.random.choice(np.array([x for x in df.index.values if x not in df_sampled.index.values]),1000,replace= False,)]\n",
        "df_test"
      ],
      "metadata": {
        "id": "65AInGblVWnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "zNcfLNKFVoEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TOKENIZER FOR ENCODER INPUT\n",
        "tk_inp = Tokenizer()\n",
        "tk_inp.fit_on_texts(df_train.enc_input.apply(str))"
      ],
      "metadata": {
        "id": "YG26pBWtVlUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZER FOR DECODER INPUT\n",
        "tk_out = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' )\n",
        "tk_out.fit_on_texts(df_train.dec_input.apply(str))"
      ],
      "metadata": {
        "id": "dqtvPJvVVmo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## THIS CLASS CONVERTS TEXT DATA TO INTEGER SEQUENCES AND RETURNS THE PADDED SEQUENCES\n",
        "class Dataset :\n",
        "    def __init__(self, data , tk_inp ,tk_out, max_len):\n",
        "        ## SETTING THE REQUIRED ATTRIBUTES\n",
        "        self.encoder_inp = data[\"enc_input\"].apply(str).values\n",
        "        self.decoder_inp = data[\"dec_input\"].apply(str).values\n",
        "        self.decoder_out = data[\"dec_output\"].apply(str).values\n",
        "        self.tk_inp = tk_inp\n",
        "        self.tk_out = tk_out\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self,i):\n",
        "        # INPUT SEQUENCES\n",
        "        self.encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n",
        "        # DECODER INPUT SEQUENCES \n",
        "        self.decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n",
        "        # DECODER INPUT SEQUENCES\n",
        "        self.decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n",
        "        \n",
        "        # PADDING THE ENCODER INPUT SEQUENCES\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, padding=\"post\",maxlen = self.max_len)\n",
        "        # PADDING THE DECODER INPUT SEQUENCES\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, padding=\"post\",maxlen = self.max_len)\n",
        "        # PADDING DECODER OUTPUT SEQUENCES\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq ,padding=\"post\", maxlen = self.max_len)\n",
        "        ##  RETURNING THE ENCODER INPUT , DECODER INPUT , AND DECODER OUTPUT\n",
        "        return self.encoder_seq ,  self.decoder_inp_seq,  self.decoder_out_seq\n",
        "    \n",
        "    def __len__(self):\n",
        "        # RETURN THE LEN OF INPUT ENDODER\n",
        "        return len(self.encoder_inp)"
      ],
      "metadata": {
        "id": "w3-qOi0IVu0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## THIS CLASS CONVERTES THE DATASET INTO THE REQUIRED BATCH SIZE\n",
        "\n",
        "class Dataloader(tf.keras.utils.Sequence):\n",
        "    def __init__(self,batch_size,dataset):\n",
        "        # INTIALIZING THE REQUIRED VARIABLES \n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.totl_points = self.dataset.encoder_inp.shape[0]\n",
        "        \n",
        "    def __getitem__(self,i):\n",
        "        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n",
        "        start = i * self.batch_size\n",
        "        stop = (i+1)*self.batch_size\n",
        "        \n",
        "        # PLACEHOLDERS FOR BATCHED DATA\n",
        "        batch_enc =[]\n",
        "        batch_dec_input = []\n",
        "        batch_dec_out =[]\n",
        "\n",
        "        for j in range(start,stop): \n",
        "            \n",
        "            a,b,c = self.dataset[j] \n",
        "            batch_enc.append(a[0]) \n",
        "            batch_dec_input.append(b[0])\n",
        "            batch_dec_out.append(c[0]) \n",
        "        \n",
        "        # Conveting list to array   \n",
        "        batch_enc = (np.array(batch_enc)) \n",
        "        batch_dec_input = np.array(batch_dec_input)\n",
        "        batch_dec_out = np.array(batch_dec_out)\n",
        "        \n",
        "        return [batch_enc , batch_dec_input],batch_dec_out\n",
        "    \n",
        "    def __len__(self):\n",
        "        # Returning the number of batches\n",
        "        return int(self.totl_points/self.batch_size)"
      ],
      "metadata": {
        "id": "59yggfH-Vxx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FORMING OBJECTS OF DATASET AND DATALOADER FOR TRAIN DATASET\n",
        "train_dataset = Dataset(df_train,tk_inp,tk_out,35)\n",
        "train_dataloader = Dataloader( batch_size = 200, dataset=train_dataset)\n",
        "\n",
        "# FORMING OBJECTS OF DATASET AND DATALOADER FOR VALIDATION DATASET\n",
        "val_dataset = Dataset(df_val , tk_inp,tk_out,35)\n",
        "val_dataloader = Dataloader(batch_size=200 , dataset=val_dataset)"
      ],
      "metadata": {
        "id": "YY9jRPX9VztF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOADING THE TENSORFLOW LIBRARIES\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ],
      "metadata": {
        "id": "b1tC847MV1cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "id": "A7lRaCufhI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# import os\n",
        "# import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "8xkbeT4SxUgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# # This is the TPU initialization code that has to be at the beginning.\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "metadata": {
        "id": "sTf0eTy-x4q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "# b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "# with tf.device('/TPU:0'):\n",
        "#   c = tf.matmul(a, b)\n",
        "\n",
        "# print(\"c device: \", c.device)\n",
        "# print(c)"
      ],
      "metadata": {
        "id": "CVIbjaTSyGOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
      ],
      "metadata": {
        "id": "lvzos7pbhMCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEFINING THE ENCODER LAYER AS A FUNCTION\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, vocab_size,emb_dims, enc_units, input_length,batch_size):\n",
        "        super().__init__()\n",
        "        # INITIALIZING THE REQUIRED VARIABLES\n",
        "        self.batch_size=batch_size # BATHCH SIZE\n",
        "        self.enc_units = enc_units # ENCODER UNITS\n",
        "\n",
        "        # EMBEDDING LAYER\n",
        "        self.embedding= layers.Embedding(vocab_size ,emb_dims) \n",
        "        # LSTM LAYER WITH RETURN SEQ AND RETURN STATES\n",
        "        self.lstm = layers.LSTM(self.enc_units,return_state= True,return_sequences =  True) \n",
        "\n",
        "    def call(self, enc_input , states):\n",
        "        \n",
        "        # FORMING THE EMBEDDED VECTOR \n",
        "        emb = self.embedding(enc_input)\n",
        "        # PASSING THE EMBEDDED VECTIO THROUGH LSTM LAYERS \n",
        "        enc_output,state_h,state_c = self.lstm(emb,initial_state=states)\n",
        "        #RETURNING THE OUTPUT OF LSTM LAYER\n",
        "        return enc_output,state_h,state_c \n",
        "\n",
        "    def initialize(self,batch_size):\n",
        "      \n",
        "        return tf.zeros(shape=(batch_size,self.enc_units)),tf.zeros(shape=(batch_size,self.enc_units))\n"
      ],
      "metadata": {
        "id": "rwi9k1HAV5ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS IS ATTNETION LAYER FOR DOT MODEL\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "  \n",
        "    '''THIS FUNCTION RETURNS THE CONTEXT VECTOR AND ATTENTION WEIGHTS (ALPHA VALUES)'''\n",
        "    def __init__(self,units):\n",
        "        super().__init__()\n",
        "        # INITIALIZING THE NUMBER OF UNITS IN ATTENTION MODEL\n",
        "        self.units =  units\n",
        "\n",
        "    def call(self,enc_output,dec_state):\n",
        "        # EXPANDING THE DIMENSION OF DECODER STATE  EG. FROM (16,32) TO (16,32,1)\n",
        "        dec_state =  tf.expand_dims(dec_state,axis=-1)\n",
        "        \n",
        "        # MATRIX MULTIPLICATION OF ENCODER OUTPUT AND MODIFIED DECODER STATE\n",
        "        # (16,32,1)*(16,13,32) = (16,13,1)\n",
        "        score = tf.matmul(enc_output,dec_state)\n",
        "        \n",
        "        # APPLYING SOFTMAX TO THE AXIS 1\n",
        "        # OUPUT SHAPE = (16,13,1)\n",
        "        att_weights = tf.nn.softmax(score,axis=1)\n",
        "        \n",
        "        # CALCULATING THE CONTEXT VECTOR BY FIRST ELEMENTWISE MULTIPLICATION AND THEN ADDING THE AXIS 1\n",
        "        # (16,13,1)*(16,13,32)=(16,13,32)\n",
        "        context_vec  = att_weights* enc_output\n",
        "        # (16,13,32) SUM AND REDUCE THE DIMENSION AT AXIS 1 => (16,32)\n",
        "        context_vec = tf.reduce_sum(context_vec,axis=1)\n",
        "        \n",
        "        # RETURNING THE CONTEXT VECTOR AND ATTENTION WEIGHTS\n",
        "        return context_vec,att_weights"
      ],
      "metadata": {
        "id": "nveQQ8owV7aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Onestepdecoder(tf.keras.Model):\n",
        "    '''THIS MODEL OUTPUTS THE RESULT OF DECODER FOR ONE TIME SETP GIVEN THE INPUT FOR PRECIOVE TIME STEP'''\n",
        "    \n",
        "    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n",
        "        super().__init__()\n",
        "        # INTITALIZING THE REQUIRED VARIABLES\n",
        "        \n",
        "        # EMBEDDING LAYERS\n",
        "        self.emb = layers.Embedding(vocab_size,emb_dims,input_length= input_len)\n",
        "        # ATTENTION LAYER\n",
        "        self.att = Attention(att_units)\n",
        "        # LSTM LAYER\n",
        "        self.lstm = layers.LSTM(dec_units,return_sequences=True,return_state=True)\n",
        "        # DENSE LAYER\n",
        "        self.dense = layers.Dense(vocab_size,activation=\"softmax\")\n",
        "\n",
        "    def call(self, encoder_output , input , state_h,state_c):\n",
        "        # FORMING THE EMBEDDED VECTOR FOR THE WORD\n",
        "        # (32,1)=>(32,1,12)\n",
        "        emb = self.emb(input)\n",
        "\n",
        "        dec_output,dec_state_h,dec_state_c = self.lstm(emb, initial_state = [state_h,state_c] )\n",
        "\n",
        "        # GETTING THE CONTEXT VECTOR AND ATTENTION WEIGHTS BASED ON THE ENCODER OUTPUT AND  DECODER STATE_H\n",
        "        context_vec,alphas = self.att(encoder_output,dec_state_h)\n",
        "        \n",
        "        # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n",
        "        dense_input =  tf.concat([tf.expand_dims(context_vec,1),dec_output],axis=-1)\n",
        "        \n",
        "        # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n",
        "        fc = self.dense(dense_input)\n",
        "        \n",
        "        # RETURNING THE OUTPUT\n",
        "        return fc , dec_state_h , dec_state_c , alphas"
      ],
      "metadata": {
        "id": "dr-j0lZiV--w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    '''THIS MODEL PERFORMS THE WHOLE DECODER OPERATION FOR THE COMPLETE SENTENCE'''\n",
        "    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n",
        "        super().__init__()\n",
        "        # INITIALIZING THE VARIABLES\n",
        "        # LENGTH OF INPUT SENTENCE\n",
        "        self.input_len = input_len\n",
        "        # ONE STEP DECODER\n",
        "        self.onestepdecoder = Onestepdecoder(vocab_size,emb_dims, dec_units, input_len,att_units,batch_size)\n",
        "\n",
        "    def call(self,dec_input,enc_output,state_h,state_c):\n",
        "        # THIS VATIABLE STORES THE VALUE OF STATE_H FOR THE PREVIOUS STATE\n",
        "        current_state_h = state_h \n",
        "        current_state_c = state_c\n",
        "        # THIS STORES THE DECODER OUTPUT FOR EACH TIME STEP\n",
        "        pred = []\n",
        "        # THIS STORED THE ALPHA VALUES\n",
        "        alpha_values = []\n",
        "        # FOR EACH WORD IN THE INPUT SENTENCE\n",
        "        for i in range(self.input_len):\n",
        "            \n",
        "            # CURRENT WORD TO INPUT TO ONE STEP DECODER\n",
        "            current_vec = dec_input[:,i]\n",
        "\n",
        "            # EXPANDING THE DIMENSION FOR THE WORD\n",
        "            current_vec = tf.expand_dims(current_vec,axis=-1)\n",
        "\n",
        "            # PERFORMING THE ONE STEP DECODER OPERATION \n",
        "            dec_output,dec_state_h,dec_state_c,alphas = self.onestepdecoder(enc_output ,current_vec,current_state_h,current_state_c)\n",
        "\n",
        "            #UPDATING THE CURRENT STATE_H\n",
        "            current_state_h = dec_state_h\n",
        "            current_state_c = dec_state_c\n",
        "\n",
        "            #APPENDING THE DECODER OUTPUT TO \"pred\" LIST\n",
        "            pred.append(dec_output)\n",
        "\n",
        "            # APPENDING THE ALPHA VALUES\n",
        "            alpha_values.append(alphas)\n",
        "            \n",
        "        # CONCATINATING ALL THE VALUES IN THE LIST\n",
        "        output = tf.concat(pred,axis=1)\n",
        "        # CONCATINATING ALL THE ALPHA VALUES IN THE LIST\n",
        "        alpha_values = tf.concat(alpha_values,axis = -1)\n",
        "        # RETURNING THE OUTPUT\n",
        "        return output , alpha_values"
      ],
      "metadata": {
        "id": "s-qWag8aWBjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "    '''THIS MODEL COMBINES ALL THE LAYERS AND FORM IN ENCODER DECODER MODEL WITH ATTENTION MECHANISM'''\n",
        "    def __init__(self,enc_vocab_size,enc_emb_dim,enc_units,enc_input_length,\n",
        "             dec_vocab_size,dec_emb_dim,dec_units,dec_input_length ,att_units, batch_size):\n",
        "        # INITAILIZING ALL VARIABLES\n",
        "        super().__init__()\n",
        "        # BATCH SIZE\n",
        "        self.batch_size = batch_size\n",
        "        # INITIALIZING ENCODER LAYER\n",
        "        self.encoder = Encoder(enc_vocab_size, enc_emb_dim,enc_units, enc_input_length,batch_size)\n",
        "        # INITALIZING DECODER LAYER\n",
        "        self.decoder = Decoder(dec_vocab_size ,dec_emb_dim,dec_units,dec_input_length  ,att_units, batch_size)\n",
        "\n",
        "    def call(self,data):\n",
        "        # THE INPUT OF DATALOADER IS IN A LIST FORM FOR EACH BATCH IT GIVER TWO INPUTS\n",
        "        # INPUT1 IS FOR ENCODER\n",
        "        # INPUT2 IS FOR DECODER\n",
        "        inp1 , inp2 = data\n",
        "        # PASSING THE INPUT1 TO ENCODER LAYER\n",
        "        enc_output, enc_state_h, enc_state_c = self.encoder(inp1,self.encoder.initialize(self.batch_size))\n",
        "        # PASSING INPUT2 TO THE DECODER LAYER\n",
        "        dec_output , alphas = self.decoder(inp2 , enc_output,enc_state_h,enc_state_c)\n",
        "        # THE OUTPUT OF MODEL IS ONLY DECODER OUTPUT THE ALPHA VALUES ARE IGNORED HERE\n",
        "        return dec_output"
      ],
      "metadata": {
        "id": "DIZ2e0ozWCPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITAILZING THE MODEL\n",
        "model = encoder_decoder(enc_vocab_size=len(tk_inp.word_index)+1,\n",
        "                         enc_emb_dim = 300,\n",
        "                         enc_units=256,enc_input_length=35,\n",
        "                         dec_vocab_size =len(tk_out.word_index)+1,\n",
        "                         dec_emb_dim =300,\n",
        "                         dec_units=256,\n",
        "                         dec_input_length = 35,\n",
        "                         \n",
        "                         att_units=256,\n",
        "                         batch_size=200)"
      ],
      "metadata": {
        "id": "xhsBCWNPWFcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEFINING THE CALLBACKS\n",
        "callback =[ tf.keras.callbacks.ModelCheckpoint( \"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/besh.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n",
        "           tf.keras.callbacks.EarlyStopping(patience=5,verbose=1,min_delta=0.0001),\n",
        "            tf.keras.callbacks.TensorBoard(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/logs/save\",histogram_freq=1)\n",
        "]\n",
        "## STORING THE NUMBER OF STEPS IN ONE EPOCH FOR TRAIN AND VALIDATION DATASET\n",
        "train_steps = train_dataloader.__len__()\n",
        "val_steps  = val_dataloader.__len__()\n",
        "# COMPILING THE MODEL\n",
        "loss= tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer= tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')\n",
        "# model.compile(optimizer=optimizer,loss=loss)"
      ],
      "metadata": {
        "id": "zOor_yFcWqnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FITTING THE MODEL\n",
        "model.fit(train_dataloader, steps_per_epoch=train_steps, epochs= 50, validation_data = val_dataloader, validation_steps =val_steps, callbacks=callback)"
      ],
      "metadata": {
        "id": "TXzY8EkUWtal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build([(200,35),(200,35)])"
      ],
      "metadata": {
        "id": "pgAXxv6rWxqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "model.save('grammar correction.hd5')\n",
        "print('model saved')"
      ],
      "metadata": {
        "id": "CSjq_P7kWzpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/logs/save"
      ],
      "metadata": {
        "id": "I7gP_dAIW4qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model,show_shapes=True)"
      ],
      "metadata": {
        "id": "aIlKiG2lW7je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING THE WEIGHTS FOR BEST MODEL\n",
        "model.load_weights(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/besh.h5\")"
      ],
      "metadata": {
        "id": "sJ1IRN3VW-kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## THIS FUNCTION IS USED IN THE INFERENCE TIME TO PREDICT THE RESULTS GIVEN THE INPUT TEXT\n",
        "\n",
        "def predict(ita_text,model):\n",
        "    '''THIS FUNCTION IS USED IN INFERENCE TIME WHICH GIVEN ANY SENTENCE IN ITALIAN OUTPUTS THE ENGLISH SENTENCE AND ALPHA VALUES'''\n",
        "    # FORMING TOKENIZED SEQUENCES FOR INPUT SENTENCE\n",
        "    seq = tk_inp.texts_to_sequences([ita_text])\n",
        "    # PADDING THE SEQUENCES\n",
        "    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n",
        "    # INITIALIZING THE STATES FOR INPUTING TO ENCODER\n",
        "    state = model.layers[0].initialize(1)\n",
        "    # GETTING THE ENCODED OUTPUT\n",
        "    enc_output,state_h,state_c= model.layers[0](seq,state)\n",
        "    # VARIABLE TO STORE PREDICTED SENTENCE\n",
        "    pred = []\n",
        "    # THIS VARIABLE STORES THE STATE TO BE INPUTED TO ONE STEP ENCODER\n",
        "    input_state_h = state_h\n",
        "    input_state_c = state_c\n",
        "    # THIS VARIABLE STORES THE VECTOR TO VE INPUTED TO ONE STEP ENCODER\n",
        "    current_vec = tf.ones((1,1))\n",
        "    # THIS VARIABLE WILL STORE ALL THE ALPHA VALUES OUTPUTS\n",
        "    alpha_values = []\n",
        "\n",
        "    for i in range(20):\n",
        "        # PASSING THE REQUIRED VARIABLE TO ONE STEP ENCODER LAYER\n",
        "        fc , dec_state_h ,dec_state_c, alphas = model.layers[1].layers[0](enc_output , current_vec ,input_state_h ,input_state_c)\n",
        "        #APPENDING THE ALPHA VALUES TO THE LIST \"alpha_values\"\n",
        "        alpha_values.append(alphas)\n",
        "         # UPDATING THE CURRENT VECTOR \n",
        "        current_vec = np.argmax(fc , axis = -1)\n",
        "         # UPDATING THE INPUT STATE\n",
        "        input_state_h = dec_state_h\n",
        "        input_state_c = dec_state_c\n",
        "        # GETTING THE ACTUAL WORDS FRO THE TOKENIZED INDEXES\n",
        "        pred.append(tk_out.index_word[current_vec[0][0]])\n",
        "        # IF THE WORD \"<end>\" COMES THE LOOP WILL BREAK\n",
        "        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n",
        "              break\n",
        "    # JOINING THE PREDICTED WORDS\n",
        "    pred_sent = \" \".join(pred)\n",
        "    # CONCATINATING ALL THE ALPHA VALUES\n",
        "    alpha_values = tf.squeeze(tf.concat(alpha_values,axis=-1),axis=0)\n",
        "    # RETURNING THE PREDICTED SENTENCE AND ALPHA VALUES\n",
        "    return  pred_sent , alpha_values"
      ],
      "metadata": {
        "id": "U-8Wc205XAhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu"
      ],
      "metadata": {
        "id": "3Yw2xkBgXCBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLEU = []\n",
        "np.random.seed(1)\n",
        "test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n",
        "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
        "    try:\n",
        "        pred = predict(str(i.enc_input),model)[0].split()\n",
        "        act = [str(i.dec_output).split()]\n",
        "        b = bleu.sentence_bleu(act,pred)\n",
        "        BLEU.append(b)\n",
        "    except:\n",
        "        print(ind)\n",
        "        print(i.enc_input)\n",
        "print(\"BELU = \", np.mean(BLEU))"
      ],
      "metadata": {
        "id": "FiMClWGBXDz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n",
        "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model)[0])\n",
        "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"
      ],
      "metadata": {
        "id": "2-Ui26p5XFvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n",
        "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model)[0])\n",
        "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"
      ],
      "metadata": {
        "id": "yszhfQcJXK-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "predict(df_test.enc_input.values[50],model)[0]"
      ],
      "metadata": {
        "id": "Pt6RS_tZXMc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(df_test.enc_input.values[50],model)[0]"
      ],
      "metadata": {
        "id": "kYkmEq0jvzDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(input,model,k):\n",
        "    seq = tk_inp.texts_to_sequences([input])\n",
        "    seq = pad_sequences(seq,maxlen = 35,padding=\"post\")\n",
        "\n",
        "    state = model.layers[0].initialize(1)\n",
        "    # GETTING THE ENCODED OUTPUT\n",
        "    enc_output,enc_state_h,enc_state_c = model.layers[0](seq,state)\n",
        "    \n",
        "\n",
        "    input_state_h = enc_state_h\n",
        "    input_state_c = enc_state_c \n",
        "    k_beams = [[tf.ones((1,1),dtype=tf.int32),0.0]]\n",
        "    for i in range(35):\n",
        "        candidates = []\n",
        "        for sent_pred , prob in k_beams :\n",
        "            if tk_out.word_index[\"<end>\"] in sent_pred.numpy() :\n",
        "                candidates += [[sent_pred , prob]]\n",
        "            else:\n",
        "               \n",
        "                dec_input = model.layers[1].layers[0].layers[0](sent_pred)\n",
        "                dec_output , dec_state_h , dec_state_c   =  model.layers[1].layers[0].layers[2](dec_input ,  initial_state =  [input_state_h , input_state_c])\n",
        "\n",
        "                context_vec , alphas =  model.layers[1].layers[0].layers[1](enc_output,dec_state_h)\n",
        "\n",
        "                # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n",
        "                dense_input =  tf.concat([tf.expand_dims(context_vec,1),tf.expand_dims(dec_state_h,1)],axis=-1)\n",
        "                \n",
        "                # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n",
        "                dense = model.layers[1].layers[0].layers[3](dense_input)\n",
        "\n",
        "                pred = tf.argsort(dense, direction= 'DESCENDING')[:,:,:k]\n",
        "                for w in range(k):\n",
        "                  candidates += [[tf.concat((sent_pred, pred[:,:,w]) , axis=-1) , (prob + tf.math.log(dense[:,:,pred[:,:,w][0][0]])[0][0])]  ]\n",
        "        k_beams = sorted(candidates,key=lambda tup:tup[1],reverse=True)[:k]\n",
        "\n",
        "    all_sent = []\n",
        "    for i,score in k_beams:\n",
        "        sent = \"\"\n",
        "        for j in range(1,35):\n",
        "            sent +=  tk_out.index_word[i.numpy()[:,j][0]] +  \" \" \n",
        "            if tk_out.index_word[i.numpy()[:,j][0]] ==\"<end>\":\n",
        "                break\n",
        "        all_sent.append((sent.strip(),score.numpy()))\n",
        "    return all_sent"
      ],
      "metadata": {
        "id": "Xn0pOvy-XPLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION BELU SCORE\n",
        "BLEU_beam = []\n",
        "index = []\n",
        "np.random.seed(1)\n",
        "test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n",
        "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
        "    try:\n",
        "        pred = beam_search(str(i.enc_input),model,3)[0][0].split()\n",
        "        act = [str(i.dec_output).split()]\n",
        "        b =bleu.sentence_bleu(act,pred)\n",
        "        BLEU_beam.append(b)\n",
        "    except:\n",
        "        index.append(ind)\n",
        "\n",
        "print(\"BELU Score = \",np.mean(BLEU_beam))"
      ],
      "metadata": {
        "id": "3kDYvtq6XRDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n",
        "print(\"=\"*50)\n",
        "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[19])\n",
        "print(\"=\"*50)\n",
        "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
        "bm = (beam_search(df_test.enc_input.values[19],model,3))\n",
        "for i in bm:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "d0ebsI-CXTMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n",
        "print(\"=\"*50)\n",
        "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[50])\n",
        "print(\"=\"*50)\n",
        "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
        "bm = (beam_search(df_test.enc_input.values[50],model,3))\n",
        "for i in bm:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "FRVU7suEXVrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.dec_output.values[50]\n",
        "df_test.enc_input.values[50]"
      ],
      "metadata": {
        "id": "HvNN_7z3va2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm = (beam_search('i ate an apple <end>',model,3))\n",
        "for i in bm:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "wa3Y-zrmvRSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s1 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/char_trainable_embedding/besh.h5\")\n",
        "# s2 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_trainable_embedding/besh.h5\")\n",
        "# s3 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_w2v/besh.h5\")\n",
        "# s4 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/word_ft/besh.h5\")\n",
        "# s5 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/bidirectional_train_emb/besh.h5\")\n",
        "# s6 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/multi_layered_word/besh.h5\")\n",
        "# s7 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_dot/besh.h5\")\n",
        "# s8 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_gernal/best.h5\")\n",
        "# s9 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/attention_concat/best.h5\")\n",
        "# s10 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monitonic_attention_dot/best.h5\")\n",
        "# s11 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_general/best.h5\")\n",
        "# s12 = os.path.getsize(\"/content/drive/MyDrive/ColabNotebooks/cs2/model_save/monotonic_attention_concat/best.h5\")"
      ],
      "metadata": {
        "id": "TPyCoJxlXZ-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_comp = pd.DataFrame()\n",
        "# df_comp[\"Model\"] = [\"Encoder Decoder(Char Level)\",\"Encoder Decoder\",\"Encoder Decoder\",\"Encoder Decoder\",\"Bidirectional Encoder Decoder\",\"Multilayered Encoder Decoder\",\"Attention Dot Model\",\"Attention Gernal Model\",\"Attention Concat Model\",\"Monotonic Attention Dot Model\",\"Monotonic Attention Gernal Model\",\"Monotonic Attention Concat Model\"]\n",
        "# df_comp[\"Embedding\"] = [\"One Hot Encoding\",\"Trainable Embedding\" , \"Pretrained Word2Vec \" ,\"Fast Text\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\",\"Trainable Embedding\"]\n",
        "# df_comp[\"BLEU Score(Greedy Search)\"] = [0.3139,0.4603,0.4453,0.4569,0.4509,0.4527, 0.5055,0.5545,0.5388,0.5469,0.5514,0.5348]\n",
        "# df_comp[\"BLEU Score(Beam Search)\"] = [\"-\",\"-\",\"-\",\"-\",0.4561,0.4557,0.5411,0.5324,0.5671,\"-\",\"-\",\"-\"]\n",
        "# df_comp[\"Model Size(bytes)\"] = [s7]\n",
        "# df_comp[\"Model Parameters\"] = [\"616,488\t\",\"26,363,578\" , \"8,158,378\", \"8,158,378\",\"35,018,938\" ,\" 28,464,826\",\"33,353,914\",\"33,419,706\",\"33,485,755\",\"33,353,914\",\"33,419,706\",\"33,485,755\"]\n",
        "# df_comp[\"Inference Time(ms)\"] = [143,92.3 , 94.5 , 93.7,250,311,157,164,189,164,179,176]\n",
        "# df_comp"
      ],
      "metadata": {
        "id": "vy9cV6AkXhF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "import tempfile\n",
        "import keras.models\n",
        "\n",
        "def make_keras_picklable():\n",
        "    def __getstate__(self):\n",
        "        model_str = \"\"\n",
        "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
        "            keras.models.save_model(self, fd.name, overwrite=True)\n",
        "            model_str = fd.read()\n",
        "        d = { 'model_str': model_str }\n",
        "        return d\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
        "            fd.write(state['model_str'])\n",
        "            fd.flush()\n",
        "            model = keras.models.load_model(fd.name)\n",
        "        self.__dict__ = model.__dict__\n",
        "\n",
        "\n",
        "    cls = keras.models.Model\n",
        "    cls.__getstate__ = __getstate__\n",
        "    cls.__setstate__ = __setstate__\n",
        "\n",
        "make_keras_picklable()"
      ],
      "metadata": {
        "id": "y8JKZVjQs-65"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}